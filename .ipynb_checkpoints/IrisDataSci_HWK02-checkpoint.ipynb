{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question1: Decision Tree for Weather Forecasting:\n",
    "Suppose that we want to build a decision tree classifier to perform weather forecasting! We have 3 features (Temp, Humidity, Wind), and a binary Label (sunny/rainy). The following table includes the data collected over the past two weeks.\n",
    "Based on this data, which feature is the best feature to put on the top of the tree? Justify your answer by providing detailed Entropy calculations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before splitting:  14 data samples, 7 sunny, 7 rainy \n",
    "Probablity of sunny :7/14\n",
    "Probability of rainy : 7/14\n",
    "\n",
    "= -[(7/14)log2(7/14) +(7/14)log2(7/14)]\n",
    "\n",
    "=1.00 bit\n",
    "\n",
    "Split based on Wind : ( 2 branches ) \n",
    "Windy: 3 Samples : 1 sunny, 2 rainy \n",
    "Not Windy : 4 samples : 3 sunny, 1 rainy\n",
    "\n",
    "Windy :\n",
    "H(x)= -[(1/3)log2(1/3) +(23)log2(2/3)]\n",
    "          = 0.9183 = 0.92\n",
    "\n",
    "Not Windy :\n",
    "H(x)= -[(3/4)log2(3/4) +(1/4)log2(1/4)]\n",
    "          = 0.8113 = 0.81\n",
    "Weighted Average: E(H(x)) = [(3/7)0.92 +(4/7)0.81]\n",
    "= 0.8571 = 0.86 bit\n",
    " \n",
    "Split based on Humidity: ( 3 branches ) \n",
    "\n",
    "• Split based on Humidity (3 branches):\n",
    "high: 5 samples: 5 Sunny, 0 Rainy\n",
    "mild: 5 samples: 3 Sunny, 2 Rainy\n",
    "low: 4 samples: 1 Rainy, 3 Sunny\n",
    "\n",
    "High:\n",
    "H(x)= -[(4/5)log2(4/5) +(15)log2(1/5)] = 0.7219 = 0.72\n",
    "\n",
    "Mild:\n",
    "H(x)= -[(3/5)log2(3/5) +(2/5)log2(2/5)] = 0.9709 = 0.97\n",
    "\n",
    "Low:\n",
    "H(x)= -[(1/4)log2(1/4) +(3/4)log2(3/4)] = 0.8112  = 0.81\n",
    "\n",
    "\n",
    "Weighted Average:\n",
    "E(H(x)) = [(5/14)0.72 +(5/14)0.97 +(4/14)0.81]  = 0.835\n",
    "\n",
    "• Split based on Temp (3 branches):\n",
    "High: 5  samples: 1 Sunny, 4 Rainy\n",
    "Mild: 4 samples: 2 Sunny, 2 Rainy\n",
    "low: 5 samples: 5 Rainy, 0 Sunny\n",
    "High:\n",
    "H(x)= -[(1/5)log2(1/5) +(4/5)log2(4/5)] = 0.7219 = 0.72 bit\n",
    "\n",
    "Mild:\n",
    "H(x)= -[(2/4)log2(2/4) +(2/4)log2(2/4)] = 1.00 = 1.00 bit\n",
    "\n",
    "Low:\n",
    "H(x)= -[(5/5)log2(5/5) +(0/5)log2(0/5)] = 0.00 bit \n",
    "Weighted Average:\n",
    "E(H(x)) = [(514)0.72 +(414)1.00 +(514)0.00]  = 0.5428 = 0.54\n",
    "\n",
    "WIND\n",
    "Entropy  Before Split : 1.00\n",
    "Entropy After Split  0.86\n",
    "Information Gain : 1.00 - 0.86 = 0.14\n",
    "\n",
    "HUMITIDITY\n",
    "Entropy  Before Split : 1.00\n",
    "Entropy After Split  0.835\n",
    "Information Gain : 1.00 - 0.84 =0.26\n",
    "\n",
    "TEMP\n",
    "Entropy  Before Split : 1.00\n",
    "Entropy After Split  0.54\n",
    "Information Gain : 1.00 - 0.54 =0. 46\n",
    "\n",
    "Based on the entropy calculations and applying it to the information gain equation, using Temp at the top of the tree will allow better prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a -Read the iris dataset from the following URL:\n",
    "#https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv \n",
    "#and assign it to a Pandas DataFrame as you learned in tutorial Lab2-3.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b- Split the dataset into testing and training sets with the following parameters: test_size=0.4, random_state=10\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "X = iris_df[feature_cols]\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#c -c- Instantiate a KNN object with K=3, train it on the training set and \n",
    "#test it on the testing set. Then, calculate the accuracy of your prediction as you learned in Lab3.\n",
    "\n",
    "\n",
    "\n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is equal to 1\n",
      "accuracy for k =  1\n",
      "0.9166666666666666\n",
      "\n",
      "\n",
      "k is equal to 5\n",
      "accuracy for k =  5\n",
      "0.95\n",
      "\n",
      "\n",
      "k is equal to 7\n",
      "accuracy for k =  7\n",
      "0.9666666666666667\n",
      "\n",
      "\n",
      "k is equal to 15\n",
      "accuracy for k =  15\n",
      "0.95\n",
      "\n",
      "\n",
      "k is equal to 27\n",
      "accuracy for k =  27\n",
      "0.95\n",
      "\n",
      "\n",
      "k is equal to 59\n",
      "accuracy for k =  59\n",
      "0.6666666666666666\n",
      "\n",
      "\n",
      "[0.9166666666666666, 0.95, 0.9666666666666667, 0.95, 0.95, 0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "# d Repeat part (c) for K=1, K=5, K=7, K=15, K=27, K=59 (you can simply use a “for loop,” and save the final accuracy results in a list).\n",
    "#Does the accuracy always get better by increasing the value K?\n",
    "k_nums = [1,5,7,15,27,59]\n",
    "list =[];\n",
    "for x in k_nums:\n",
    "\n",
    "    my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=x) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "    my_knn_for_cs4661.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Testing on the testing set:\n",
    "\n",
    "\n",
    "    print (\"k is equal to\", x)\n",
    "    y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "    #print(y_predict)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    print(\"accuracy for k = \",x)\n",
    "    print(accuracy)\n",
    "    print('\\n')\n",
    "    list.append(accuracy)\n",
    "    \n",
    "print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acccuracy doesnt not increase as you increase k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for  sepal_length   0.75\n",
      "accuracy for  sepal_width   0.6166666666666667\n",
      "accuracy for  petal_length   0.95\n",
      "accuracy for  petal_width   0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#e- Now, suppose that we would like to make prediction based on only ONE single feature. To find the best single feature, we have to try every feature individually. In other word, we have to repeat part (c) with K=11,\n",
    "#four times (each time using only one of the 4 features), and compute the accuracy each time. Then, check the accuracies.\n",
    "#Which individual feature provide the best accuracy (the best feature)? Which one is the second best feature? (Note: you have to train, test, and evaluate your model 4 times, each time on a dataset including only one of the features, and save the final accuracy results in a list).\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "\n",
    "feature_col = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "for feature in feature_col:\n",
    "    X = iris_df[[feature]]\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)  \n",
    "    k = 11\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "    print ('accuracy for ',feature , ' ',accuracy)\n",
    "\n",
    "\n",
    "   # print (accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best feature is the petal_width feature as it has  96% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for  ['sepal_length', 'sepal_width']   0.8333333333333334\n",
      "accuracy for  ['sepal_length', 'petal_length']   0.9666666666666667\n",
      "accuracy for  ['sepal_length', 'petal_width']   0.9666666666666667\n",
      "accuracy for  ['sepal_width', 'petal_length']   0.9666666666666667\n",
      "accuracy for  ['sepal_width', 'petal_width']   0.9666666666666667\n",
      "accuracy for  ['petal_length', 'petal_width']   0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "# f\n",
    "#Now, we want to repeat part (e) (with K=11), this time using two features. \n",
    "#you have to train, test, and evaluate your model for 6 different cases: using\n",
    "#(1st and 2nd features), (1st and 3rd features), \n",
    "#(1st and 4th features), (2nd and 3rd features), \n",
    "#(2nd and 4th features), (3rd and 4th features)!\n",
    "#Which “feature pair” provides the best accuracy?\n",
    "\n",
    "pair_features12 = [feature_cols[0],feature_cols[1]]\n",
    "pair_features13 = [feature_cols[0],feature_cols[2]]\n",
    "pair_features14 = [feature_cols[0],feature_cols[3]]\n",
    "pair_features23 = [feature_cols[1],feature_cols[2]]\n",
    "pair_features24 = [feature_cols[1],feature_cols[3]]\n",
    "pair_features34 = [feature_cols[2],feature_cols[3]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pair_features = [pair_features12,pair_features13,pair_features14,pair_features23,pair_features24,pair_features34]\n",
    "\n",
    "for feature in pair_features:\n",
    "    X = iris_df[feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)\n",
    "    k = 11\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    print ('accuracy for ',feature , ' ',accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best pair feature as show is the petal_length', 'petal_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#g Big Question: Does the “best feature pair” from part (f) contain of both “first best feature” \n",
    "#and “second best feature” from part (e)? In other word, can we conclude that the “best two features” for classification are the first \n",
    "#best feature along with the second best feature together\n",
    "\n",
    "\n",
    "After the analysis with the data predictions from the pair features it can be concluded that the two best features for classification are the first best feature along with second best feature. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
